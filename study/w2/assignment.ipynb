{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2주차 스터디 자료에서 MC, TD TD-$\\lambda$에 대해서 알 수 있었습니다.\n",
    "각각 특징이 있는데 다음과 같습니다.\n",
    "\n",
    "|         | MC    | TD    | TD-$\\lambda$ |\n",
    "|---------|------|------|--------------|\n",
    "| update  | offline | online | online |\n",
    "| target  | $G_t$ | $G_t^{(n)}$ | $G_t^\\lambda $ |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Evaluation 구현하기\n",
    "\n",
    "### Highway game 학습해보기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment\n",
    "\n",
    "**모든 observation은 encoding된 형태로 리턴합니다!**\n",
    "policy evaluation 알고리즘은 tabula 형태로 동작하고, 따라서 각 index를 observation 으로 지정하기 때문입니다.\n",
    "(ex: 가위 = 1번 obs, 바위 = 2번 obs, 보 = 3번 obs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from stable_baselines3.common.type_aliases import GymObs, GymStepReturn\n",
    "\n",
    "import time\n",
    "\n",
    "class HighwayGame(gym.Env):\n",
    "    def __init__(self, n_lanes: int = 3, max_steps: int = 60):\n",
    "        \"\"\"\n",
    "        클래스의 기본적인 세팅을 초기화 합니다.\n",
    "        (왠만하면) 이 클래스에서 쓰일 모든 변수는 여기서 초기화 돼야 합니다.\n",
    "        Env class는 observation_space와 action_space가 필수적으로 직접 초기화 되어야 합니다.\n",
    "\n",
    "        Parameters\n",
    "        --------------------\n",
    "        :param n_lanes: number of lanes for Highway game\n",
    "        :param max_steps: maximum steps of environment = length of the highway\n",
    "        \"\"\"\n",
    "        self.n_lanes = n_lanes\n",
    "        self.max_steps = max_steps\n",
    "        # self.road = np.load(os.path.join(PROJECT_PATH, \"w1/data/road.npy\"))  # 미리 준비해 둔 highway 맵\n",
    "        self.road = self.generate_road()\n",
    "        self.steps = 0      # 현재 steps = in-game time\n",
    "        self.current_lane = n_lanes//2      # 맨 처음에 player는 lane의 중간에 위치합니다.\n",
    "\n",
    "        self.observation_space = gym.spaces.Discrete(n_lanes)   # (0 ~ n_lane - 1) 까지의 정수\n",
    "        self.action_space = gym.spaces.Discrete(n_lanes)        # (0 ~ n_lane - 1) 까지의 정수\n",
    "\n",
    "        self.reset()    # Environment를 작동시키기 위해 초기화\n",
    "\n",
    "    def get_obs(self) -> GymObs:\n",
    "        \"\"\"\n",
    "        현재 관찰 가능한 상태를 리턴합니다.\n",
    "        ** state와 obs는 다릅니다! obs는 agent에게 직접 제공되는 state 중 하나로,\n",
    "        \"\"\"\n",
    "        return self.road[min(self.steps + 1, self.max_steps - 1)]\n",
    "\n",
    "    def get_encoded_obs(self) -> GymObs:\n",
    "        \"\"\"\n",
    "        현재 관찰 가능한 상태를 정수 형태로 인코딩 해서 리턴합니다. (only for discrete obs space)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.argmin(self.get_obs())\n",
    "\n",
    "    def reset(self) -> GymObs:\n",
    "        self.road = self.generate_road()\n",
    "        self.steps = 0\n",
    "        self.current_lane = self.n_lanes//2\n",
    "        # return self.get_obs()\n",
    "        return self.get_encoded_obs()\n",
    "\n",
    "    def step(self, action: int, debug=False) -> GymStepReturn:\n",
    "        reward, done = 0, False\n",
    "        info = dict()\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        if action < 0 or action >= self.n_lanes:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            info['msg'] = \"Invalid action\"\n",
    "\n",
    "        elif self.road[self.steps, action]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            info['msg'] = \"You crashed!\"\n",
    "\n",
    "        elif self.steps == self.max_steps - 1:\n",
    "            reward = 1\n",
    "            done = True\n",
    "            info['msg'] = \"Finished!\"\n",
    "\n",
    "        else:\n",
    "            self.current_lane = action\n",
    "\n",
    "        if debug:\n",
    "            self.render()\n",
    "            print(f\"Action: {action}, Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "\n",
    "        return self.get_encoded_obs(), reward, done, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        clear_output(wait=True)\n",
    "        for i, lane in enumerate(self.road[self.steps:self.steps+5]):\n",
    "            repr_str = [\"X\" if l else \" \" for l in lane]\n",
    "            if i == 0:\n",
    "                repr_str[self.current_lane] = \"O\"\n",
    "            print(\"|\"+\" \".join(repr_str)+\"|\")\n",
    "\n",
    "    def generate_road(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Highway game의 맵을 생성합니다.\n",
    "        :param n_lanes: number of lanes\n",
    "        :param max_steps: length of the highway\n",
    "        :return: np.ndarray, shape = (max_steps, n_lanes)\n",
    "        \"\"\"\n",
    "        road = np.ones((self.max_steps, self.n_lanes), dtype=bool)\n",
    "        for i in range(self.max_steps):\n",
    "            if i > 0 and road[i-1].any():\n",
    "                road[i, :] = 0\n",
    "            elif np.random.rand() < 0.7:\n",
    "                road[i, np.random.choice(self.n_lanes)] = 0\n",
    "\n",
    "        return road\n",
    "\n",
    "    def play(self, policy=None):\n",
    "        obs = self.reset()\n",
    "        done = False\n",
    "        self.render()\n",
    "\n",
    "        while not done:\n",
    "            action = policy.action(obs) if policy else self.action_space.sample()\n",
    "            obs, reward, done, info = self.step(action)\n",
    "            self.render()\n",
    "            time.sleep(0.25)\n",
    "            if done:\n",
    "                if info['msg'] != \"Finished!\":\n",
    "                    raise ValueError('제대로 학습되지 않았습니다!')\n",
    "                print(info['msg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy Evaluation 알고리즘 준비"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from study.w2.MC_TD import *\n",
    "\n",
    "env = HighwayGame(n_lanes=3)\n",
    "vf = ValueFunction(env.observation_space, env.action_space)\n",
    "policy = GreedyPolicy(env, vf, eps=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "total_iter = 5000\n",
    "\n",
    "def eps_schedule(ep):\n",
    "    if ep < total_iter * 0.6:\n",
    "        return 0.9\n",
    "    return 0.9 ** (1 + 0.02 * ep) + 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Highway game에서, encoded_obs는 차가 없는 lane index를 의미합니다. (ex. obs=1 => 1번 lane이 비었음)\n",
    "따라서, 학습이 제대로 됐다면 value function의 diagonal 성분 (encoded_obs == action 인 지점 => 비어있는 곳으로 움직이는 action value)가 해당 row에서 가장 높아야 합니다.\n",
    "\n",
    "제대로 학습이 되는지 확인해 봅니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. MC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: -1.00\t eps: 0.010: 100%|██████████| 5000/5000 [00:02<00:00, 1839.34it/s]\n"
     ]
    }
   ],
   "source": [
    "mc = MonteCarloEvaluation(policy, eps_schedule, gamma=0.9)\n",
    "reward_logs = mc.evaluate(n_episodes=total_iter, steps=-1)      # MC는 episode 끝까지 step을 진행하므로 무제한 steps를 줘야 합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy actions: [0 1 2]\n",
      "Value function trained by MC:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.72314041, -0.89055295, -0.89663898],\n       [-1.        , -0.60504666, -1.        ],\n       [-1.        , -1.        , -0.6027867 ]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Greedy actions: {np.argmax(policy.vf.values, axis=-1)}')\n",
    "print('Value function trained by MC:')\n",
    "policy.vf.values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. n-steps TD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "vf = ValueFunction(env.observation_space, env.action_space)\n",
    "policy = GreedyPolicy(env, vf)\n",
    "td = TemporalDifferenceEvaluation(policy, eps_schedule, gamma=0.9, alpha=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: -1.00\t eps: 0.010: 100%|██████████| 5000/5000 [00:03<00:00, 1658.02it/s]\n"
     ]
    }
   ],
   "source": [
    "reward_logs = td.evaluate(total_iter, steps=3)      # 3 steps TD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "value function table을 보면, MC보다는 정확도가 떨어지는 것을 볼 수 있습니다. (diagonal 성분이 높을 수록 정확합니다.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy actions: [0 1 2]\n",
      "Value function trained by 3-steps TD:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.87640209, -0.93629915, -0.94709831],\n       [-1.        , -0.80743354, -1.        ],\n       [-1.        , -1.        , -0.80867982]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Greedy actions: {np.argmax(policy.vf.values, axis=-1)}')\n",
    "print('Value function trained by 3-steps TD:')\n",
    "policy.vf.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. TD-$\\lambda$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "vf = ValueFunction(env.observation_space, env.action_space)\n",
    "policy = GreedyPolicy(env, vf)\n",
    "td_lambda = TDLambda(policy, eps_schedule, gamma=0.9, lambda_=0.9, alpha=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: -1.00\t eps: 0.010: 100%|██████████| 5000/5000 [00:03<00:00, 1553.38it/s]\n"
     ]
    }
   ],
   "source": [
    "reward_logs = td_lambda.evaluate(n_episodes=total_iter, steps=1)        # TD-lambda는 one-step을 사용하고도 n-steps의 효과를 냅니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy actions: [0 1 2]\n",
      "Value function trained by 3-steps TD-lambda:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.91223981, -0.94076518, -0.95296594],\n       [-1.        , -0.82930186, -1.        ],\n       [-1.        , -1.        , -0.82792356]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Greedy actions: {np.argmax(policy.vf.values, axis=-1)}')\n",
    "print('Value function trained by 3-steps TD-lambda:')\n",
    "policy.vf.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 과제 2\n",
    "Monte-Carlo Evaluation 알고리즘은 구현이 되지 않아 notebook 상에서 실행되지 않습니다.\n",
    "이를 구현해서 다음 env를 학습시키세요."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "env = HighwayGame(n_lanes=5, max_steps=64)\n",
    "vf = ValueFunction(env.observation_space, env.action_space)\n",
    "policy = GreedyPolicy(env, vf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MC를 이용한 학습"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.play(policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}