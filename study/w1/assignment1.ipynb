{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1\n",
    "\n",
    "## Enviroment 기초"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gym Env를 통해 Environment 익숙해지기\n",
    "\n",
    "#### 예시: 고속도로 게임\n",
    "\n",
    "고속도로 게임은 n개의 차선을 달리는 고전게임입니다.\n",
    "각 차선에는 차량이 있을 수 있으므로 차량이 빈 차선으로 시간 내에 순발력 있게 이동하면 됩니다.\n",
    "이 게임을 gym.Env를 통해 구현해보겠습니다.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "from stable_baselines3.common.type_aliases import GymObs, GymStepReturn\n",
    "\n",
    "import os, time\n",
    "\n",
    "PROJECT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "\n",
    "class HighwayGame(gym.Env):\n",
    "    def __init__(self, n_lanes: int = 3, max_steps: int = 60):\n",
    "        \"\"\"\n",
    "        클래스의 기본적인 세팅을 초기화 합니다.\n",
    "        (왠만하면) 이 클래스에서 쓰일 모든 변수는 여기서 초기화 돼야 합니다.\n",
    "        Env class는 observation_space와 action_space가 필수적으로 직접 초기화 되어야 합니다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_lanes: int\n",
    "            Number of lanes in the highway\n",
    "        max_steps: int\n",
    "            Maximum number of steps in the game = maximum time steps\n",
    "        \"\"\"\n",
    "        self.n_lanes = n_lanes\n",
    "        self.max_steps = max_steps\n",
    "        self.road = np.load(os.path.join(PROJECT_PATH, \"w1/road.npy\"))  # 미리 준비한 highway 맵\n",
    "\n",
    "        self.steps = 0\n",
    "        self.current_lane = n_lanes//2      # 처음 위치는 가운데 lane\n",
    "\n",
    "        self.observation_space = gym.spaces.Discrete(n_lanes)   # 첫번째 ... n_lanes 번째 lane에 차가 있는지 여부\n",
    "        self.action_space = gym.spaces.Discrete(n_lanes)    # 첫번째 ... n_lanes 번째 lane 중 하나로 이동\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def get_obs(self) -> GymObs:\n",
    "        \"\"\"\n",
    "        현재 agent가 관측하는 상태를 반환합니다.\n",
    "        \"\"\"\n",
    "        return self.road[min(self.steps + 1, self.max_steps - 1)]   # agent가 관측하는 바로 앞 차선 상태\n",
    "\n",
    "    def reset(self) -> GymObs:\n",
    "        \"\"\"\n",
    "        Env를 초기화 합니다.\n",
    "        \"\"\"\n",
    "        self.steps = 0\n",
    "        self.current_lane = self.n_lanes//2\n",
    "        return self.get_obs()\n",
    "\n",
    "    def step(self, action: int, debug=False) -> GymStepReturn:\n",
    "        \"\"\"\n",
    "        Env의 한 step을 진행합니다.\n",
    "        step은 현재 상태에서 action을 취한 후, 다음 상태, reward, done, info를 반환합니다.\n",
    "\n",
    "        reward: float\n",
    "            현재 상태에서 action을 취한 후 받는 reward\n",
    "        done: bool\n",
    "            현재 상태에서 action을 취한 후 게임이 끝났는지 여부\n",
    "        info: dict\n",
    "            현재 상태에서 action을 취한 후 추가 정보\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action: int\n",
    "            Agent가 선택한 action = 이동할 lane 번호\n",
    "        debug: bool\n",
    "            디버깅을 위한 flag\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        GymStepReturn: Tuple[GymObs, float, bool, dict]\n",
    "            observation, reward, done, info\n",
    "        \"\"\"\n",
    "        reward, done = 0, False\n",
    "        info = dict()\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # action이 유효한지 확인 후 reward 계산, 게임 종료 여부 판별\n",
    "        if action < 0 or action >= self.n_lanes:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            info['msg'] = \"Invalid action\"\n",
    "\n",
    "        elif self.road[self.steps, action]!=0:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            info['msg'] = \"You crashed!\"\n",
    "\n",
    "        elif self.steps == self.max_steps - 1:\n",
    "            reward = 500\n",
    "            done = True\n",
    "            info['msg'] = \"Finished!\"\n",
    "\n",
    "        else:\n",
    "            self.current_lane = action  # action이 유효하면 action을 반영\n",
    "\n",
    "        if debug:\n",
    "            self.render()\n",
    "            print(f\"Action: {action}, Reward: {reward}, Done: {done}, Info: {info}\")\n",
    "\n",
    "        return self.get_obs(), reward, done, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Env를 시각화 합니다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode: str\n",
    "            시각화 모드\n",
    "        \"\"\"\n",
    "        clear_output(wait=True)     # Jupyter 환경의 output을 지워줍니다.\n",
    "        print(f\"Step: {self.steps}/{self.max_steps}\")\n",
    "        for i, lane in enumerate(self.road[self.steps:self.steps+5]):\n",
    "            repr_str = [\"X\" if l else \" \" for l in lane]   # lane에 차가 있으면 X, 없으면 \" \"로 표시\n",
    "            if i == 0:\n",
    "                repr_str[self.current_lane] = \"O\"   # agent의 위치는 O로 표시\n",
    "            print(\"|\"+\" \".join(repr_str)+\"|\")\n",
    "\n",
    "    def play(self, policy=None):\n",
    "        obs = self.reset()\n",
    "        done = False\n",
    "        self.render()\n",
    "\n",
    "        while not done:\n",
    "            time.sleep(0.5)\n",
    "            action = policy.action(obs) if policy else self.action_space.sample()\n",
    "            obs, reward, done, info = self.step(action)\n",
    "            self.render()\n",
    "            if done:\n",
    "                print(info['msg'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 직접 플레이 해보기\n",
    "\n",
    "- 내 위치 : O\n",
    "- 상대 차 위치 : X\n",
    "- 이동 방향 : down\n",
    "- action : 이동할 lane의 번호 = (0, 1, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UserInputPolicy:\n",
    "    @staticmethod\n",
    "    def action():\n",
    "        return int(input('Enter Action: '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = HighwayGame()\n",
    "env.play(policy=UserInputPolicy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 과제 1\n",
    "\n",
    "원하는 환경을 gym.Env를 통해 만들어 보세요!\n",
    "\n",
    "**제약조건**\n",
    "- action space는 disrete space로 구현해야 합니다.\n",
    "- get_obs() 메소드를 구혆해야 합니다.\n",
    "\n",
    "**잘 만들었는지 확인하는 방법** </br>\n",
    "\n",
    "```\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "check_env(env)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이곳에 Env를 만들어 주세요\n",
    "class Env(gym.Env):\n",
    "    def __ini__(self):\n",
    "        pass\n",
    "    def step(self, action):\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Env를 체크합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "check_env(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "만약, 만든 Env의 reward가 적절하고, 종료조건이나 기타 에러가 없다면 직접 학습해서 자동으로 Env가 움직이도록 학습할 수 있습니다.\n",
    "미리 준비된 Monte-Carlo 알고리즘으로 학습해봅니다.\n",
    "\n",
    "**주의**\n",
    "- Env의 observation_space와 action_space가 전부 discrete 해야 합니다.\n",
    "- 너무 큰 space의 경우 학습이 며칠 걸릴 가능성이 높습니다.  (action space size * obs space size < 100 권장...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from study.w2.MC_TD import MonteCarloAlgorithm, GreedyPolicy, ValueFunction\n",
    "# from w2.MC_TD import MonteCarloAlgorithm, GreedyPolicy\n",
    "\n",
    "\n",
    "vf = ValueFunction(env)\n",
    "policy = GreedyPolicy(env, vf, eps=0.9)\n",
    "\n",
    "# 아래 두 변수를 적절히 조정하면 학습에 성공할 수 있습니다.\n",
    "total_steps = 5000      # 총 학습 episode 수\n",
    "random_frac = 0.6       # 랜덤하게 action을 수행할 episode 수의 전체 episode 수에 대한 비율\n",
    "\n",
    "\n",
    "# 랜덤성을 부여하는 변수를 현재까지 학습한 episode수에 따라 조절하는 함수\n",
    "# learning rate scheduler와 비슷합니다.\n",
    "def eps_schedule(i):\n",
    "    if i < total_steps * random_frac:\n",
    "        return 0.9\n",
    "    return 0.9 ** (1 + 0.01 * i) + 0.01\n",
    "\n",
    "\n",
    "mc = MonteCarloAlgorithm(env, vf, policy, gamma=0.9)\n",
    "mc.learn(total_steps, eps_schedule=eps_schedule, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Env에 play라는 함수가 구현되어 있다면, 직접 자동으로 플레이 해볼 수 있습니다.\n",
    "\n",
    "구현이 되어 있지 않다면 다음 코드를 Env의 play 메소드로 설정해 보세요.\n",
    "\n",
    "```\n",
    "def play(self, policy=None):\n",
    "    obs = self.reset()\n",
    "    done = False\n",
    "    self.render()\n",
    "\n",
    "    while not done:\n",
    "        time.sleep(0.5)\n",
    "        action = policy.action() if policy else self.action_space.sample()\n",
    "        obs, reward, done, info = self.step(action)\n",
    "        self.render()\n",
    "        if done:\n",
    "            print(info)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.play(policy)        # env에 play라는 메소드가 있어야 합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}